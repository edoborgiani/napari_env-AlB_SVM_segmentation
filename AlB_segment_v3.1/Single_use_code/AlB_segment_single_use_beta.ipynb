{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8af2e18-3b4b-4f2e-b616-eb14b762d360",
   "metadata": {},
   "source": [
    "## PACK INSTALLATION (Restart kernel after run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea9be9-7e17-4004-a41c-9133945aaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the scikit-learn library for machine learning algorithms\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087dd87-8742-4e60-8f02-dea806bcdb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the xlsxwriter library for creating Excel files\n",
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf91cf-09e6-4c55-b644-6d4971b1215b",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1262a-3ccc-4c94-8f6e-8a5784ced14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV and check its version\n",
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61116848-a1b8-4dea-a7da-25a040cc7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing, visualization, and machine learning\n",
    "from PIL import Image\n",
    "\n",
    "# Napari for interactive visualization\n",
    "import napari\n",
    "\n",
    "# General-purpose libraries\n",
    "import scipy\n",
    "import csv\n",
    "import imghdr\n",
    "import colorsys\n",
    "import os\n",
    "import xlsxwriter\n",
    "\n",
    "# Numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# OpenCV for image processing\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyVista for 3D visualization\n",
    "import pyvista as pv\n",
    "\n",
    "# Stardist for deep learning-based segmentation\n",
    "from csbdeep.utils import normalize\n",
    "from stardist.models import StarDist2D, Config3D, StarDist3D\n",
    "from stardist.data import test_image_nuclei_2d\n",
    "from stardist.plot import render_label\n",
    "\n",
    "# Scipy and Skimage for image processing and segmentation\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import filters\n",
    "from skimage.draw import disk\n",
    "from skimage.segmentation import watershed\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.filters import threshold_local\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "# from skimage.feature import peak_local_max\n",
    "from vispy.color import Colormap\n",
    "from IPython.display import display_html\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from stardist.models import StarDist2D, Config3D, StarDist3D\n",
    "from stardist.data import test_image_nuclei_2d\n",
    "from stardist.plot import render_label\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Morphological operations for image processing\n",
    "from skimage.morphology import remove_small_holes, remove_small_objects\n",
    "from skimage.morphology import closing, square\n",
    "\n",
    "# Napari settings for interactive visualization\n",
    "from napari.settings import get_settings\n",
    "settings = get_settings()\n",
    "\n",
    "# AICSImageIO for reading and writing image files\n",
    "from aicsimageio import AICSImage\n",
    "from tifffile import imwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72018ca3-1a38-43d8-8cd8-2f2e391913d0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb3381-a03c-4ce8-b871-784e5113fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse_pixels(imarray, center, semi_axes, rotation, image_shape):\n",
    "    \"\"\"\n",
    "    Generate the pixel coordinates that lie inside an ellipse.\n",
    "\n",
    "    Parameters:\n",
    "    - center: Tuple (x, y) representing the center of the ellipse.\n",
    "    - semi_axes: Tuple (semi_major_axis, semi_minor_axis) representing the lengths of the ellipse's axes.\n",
    "    - rotation: Rotation angle of the ellipse in radians.\n",
    "    - image_shape: Shape of the image (height, width) to constrain the ellipse.\n",
    "\n",
    "    Returns:\n",
    "    - A 2D array of pixel coordinates (row, column) that lie inside the ellipse.\n",
    "    \"\"\"\n",
    "    # Create a grid of x and y coordinates for the image\n",
    "    y, x = np.meshgrid(np.arange(imarray.shape[1]), np.arange(imarray.shape[0]), indexing='xy')\n",
    "    \n",
    "    # Compute the cosine and sine of the rotation angle\n",
    "    cos_theta = np.cos(rotation)\n",
    "    sin_theta = np.sin(rotation)\n",
    "    \n",
    "    # Rotate the x and y coordinates to align with the ellipse's axes\n",
    "    x_rot = cos_theta * (x - center[0]) + sin_theta * (y - center[1])\n",
    "    y_rot = -sin_theta * (x - center[0]) + cos_theta * (y - center[1])\n",
    "\n",
    "    # Create a mask for pixels that lie inside the ellipse\n",
    "    mask = (x_rot / semi_axes[0])**2 + (y_rot / semi_axes[1])**2 <= 1\n",
    "\n",
    "    # Enable interactive mode for Napari (if needed)\n",
    "    settings.application.ipy_interactive = True\n",
    "\n",
    "    # Return the coordinates of the pixels inside the ellipse\n",
    "    return np.column_stack(np.where(mask))\n",
    "\n",
    "def rgb_to_hsv(rgb_array):\n",
    "    \"\"\"\n",
    "    Convert an array of RGB values to HSV (Hue, Saturation, Value).\n",
    "\n",
    "    Parameters:\n",
    "    - rgb_array: A 2D array where each row is an RGB triplet (R, G, B).\n",
    "\n",
    "    Returns:\n",
    "    - A 2D array where each row is an HSV triplet (H, S, V).\n",
    "      H is in degrees (0-360), S and V are percentages (0-100).\n",
    "    \"\"\"\n",
    "    hsv_list = []\n",
    "    for rgb in rgb_array:\n",
    "        r, g, b = rgb  # Extract the RGB components\n",
    "        h, s, v = colorsys.rgb_to_hsv(r, g, b)  # Convert RGB to HSV\n",
    "        hsv_list.append([h * 360, s * 100, v * 100])  # Scale H to degrees, S and V to percentages\n",
    "    return np.array(hsv_list)\n",
    "\n",
    "def csv_to_matrix(file_path):\n",
    "    \"\"\"\n",
    "    Read a CSV file and convert its contents into a matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A 2D list where each row corresponds to a row in the CSV file.\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Convert numeric values to integers, leave others as strings\n",
    "            matrix.append([int(value) if value.replace('.', '', 1).isdigit() else value for value in row])\n",
    "    return matrix\n",
    "\n",
    "def classify_points(cloud_points, test_points, bandwidth=1.0):\n",
    "    \"\"\"\n",
    "    Classify test points based on their similarity to a cloud of points using Kernel Density Estimation (KDE).\n",
    "\n",
    "    Parameters:\n",
    "    - cloud_points: A 2D array of points representing the training data.\n",
    "    - test_points: A 2D array of points to classify.\n",
    "    - bandwidth: Bandwidth parameter for the KDE (controls smoothness).\n",
    "\n",
    "    Returns:\n",
    "    - A 1D array of probabilities for each test point.\n",
    "    \"\"\"\n",
    "    # Fit a Kernel Density Estimator to the cloud points\n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "    kde.fit(cloud_points)\n",
    "    \n",
    "    # Evaluate the probability density for each test point\n",
    "    log_density = kde.score_samples(test_points)\n",
    "    \n",
    "    # Convert log-density to probabilities\n",
    "    probabilities = np.exp(log_density)\n",
    "    return probabilities\n",
    "\n",
    "def calibrate_image_resolution(image: np.ndarray):\n",
    "    \"\"\"\n",
    "    Opens a Napari viewer for interactive calibration.\n",
    "    The user draws a line over the scalebar using the Shapes layer (Line)\n",
    "    and presses 'q' to confirm. After the viewer closes the function will\n",
    "    ask for the real-world scalebar length (in Âµm) and return the pixel\n",
    "    resolution in Âµm/pixel.\n",
    "\n",
    "    This function will loop and offer a retry if no line was confirmed\n",
    "    (to avoid the need to re-run the calling cell).\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        viewer = napari.Viewer()\n",
    "        viewer.add_image(image, name='image')\n",
    "        shapes_layer = viewer.add_shapes(name='scalebar', shape_type='line')\n",
    "\n",
    "        # Provide clear on-screen instructions before opening Napari\n",
    "        settings.application.ipy_interactive = False\n",
    "        print(\"\\nðŸŸ¢ Napari opened: draw a line over the scalebar using the Shapes tool (Line).\")\n",
    "        print(\"When finished press 'q' while the Napari window is focused to confirm the measurement.\")\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        @viewer.bind_key('q')\n",
    "        def _on_confirm(event=None):\n",
    "            \"\"\"Callback triggered when 'q' is pressed.\"\"\"\n",
    "            if len(shapes_layer.data) == 0:\n",
    "                print(\"âš ï¸ No line found! Please draw one first and press 'q' again.\")\n",
    "                return\n",
    "            line_coords = shapes_layer.data[0]\n",
    "            p1, p2 = line_coords\n",
    "            pixel_length = np.linalg.norm(p2 - p1)\n",
    "            print(f\"Measured scalebar length: {pixel_length:.2f} pixels\")\n",
    "            result['pixel_length'] = pixel_length\n",
    "            viewer.close()\n",
    "\n",
    "        napari.run()  # Waits until viewer is closed\n",
    "\n",
    "        # If user confirmed a line, proceed to ask for real-world length\n",
    "        if 'pixel_length' in result:\n",
    "            try:\n",
    "                real_length_um = float(input(\"Enter the real-world scalebar length (in Âµm): \"))\n",
    "            except Exception:\n",
    "                raise ValueError(\"Invalid real-world length entered.\")\n",
    "            resolution_um_per_px = real_length_um / result['pixel_length']\n",
    "            print(f\"\\nâœ… Pixel resolution: {resolution_um_per_px:.4f} Âµm/pixel\")\n",
    "            return resolution_um_per_px\n",
    "\n",
    "        # If no line was confirmed, offer a retry to the user\n",
    "        print(\"No scalebar line was confirmed in Napari.\")\n",
    "        retry = input(\"Retry calibration? (y/n): \").strip().lower()\n",
    "        if retry != 'y':\n",
    "            raise ValueError(\"Calibration aborted by user. No line was confirmed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b34b44-3ed1-4f8e-9fb8-4d79e74194d7",
   "metadata": {},
   "source": [
    "## Training image\n",
    "Choose the image that will be used as training. It will be opened in Napari. Choose the circular selection on the right and choose as many nuclei as possible to train the algorithm. Then close the Napari window to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe7692-e008-41f6-9aa9-a2e4b016f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training file and extract its name without the extension\n",
    "training_file = 'pAC026 D28 uM SafO slide7.jpg'\n",
    "training_stem = os.path.splitext(os.path.basename(training_file))[0]\n",
    "\n",
    "# Open the TIFF file as an image\n",
    "img_train = Image.open(training_file)\n",
    "\n",
    "# Convert the image to a NumPy array for further processing\n",
    "imarray_train = np.array(img_train)\n",
    "\n",
    "# Remove alpha channel if present\n",
    "if imarray_train.shape[2] == 4:\n",
    "    imarray_train = imarray_train[:, :, :3]\n",
    "\n",
    "# Create a copy of the original image to define the ROI (Region of Interest)\n",
    "ROI_train = imarray_train.copy()\n",
    "\n",
    "# Initialize a mask image with the same dimensions as the green channel of the original image\n",
    "mask_train = np.zeros(np.shape(imarray_train[:, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf29ea-d42e-49ef-9086-48f89f99a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the investigation\n",
    "step = 30  # Size of the square for investigation\n",
    "delta = 1  # Sensitivity for blue dominance\n",
    "holes_threshold = 1000  # Minimum area to consider a hole\n",
    "island_threshold = 1000  # Minimum size to consider an object\n",
    "\n",
    "# Flag to determine whether to use an existing CSV file for training points\n",
    "use_csv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4eeaa6-1529-47c2-ba0c-d8192c69f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect average brightness and contrast from ROI of training image\n",
    "\n",
    "# Iterate through the image in steps to identify blue-dominant regions\n",
    "for i in range(1 + step, imarray_train.shape[0], step):\n",
    "    for j in range(1 + step, imarray_train.shape[1], step):\n",
    "        # Check if the blue channel is dominant over both red and green channels\n",
    "        if (\n",
    "            np.mean(ROI_train[i-step:i+step, j-step:j+step, 0]) >= delta + np.mean(ROI_train[i-step:i+step, j-step:j+step, 1]) and\n",
    "            np.mean(ROI_train[i-step:i+step, j-step:j+step, 0]) >= delta + np.mean(ROI_train[i-step:i+step, j-step:j+step, 2])\n",
    "        ):\n",
    "            # Mark the region as part of the mask\n",
    "            mask_train[i:i+step, j:j+step, :] = 1\n",
    "        \n",
    "bright_train=np.mean(imarray_train[:,:,:])\n",
    "bright_train_ROI=np.mean(imarray_train[mask_train==1])\n",
    "\n",
    "bright_train_R=np.mean(imarray_train[:,:,0])\n",
    "bright_train_G=np.mean(imarray_train[:,:,1])\n",
    "bright_train_B=np.mean(imarray_train[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56ac57-1165-4c7f-9ec9-7e97bb7bf186",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply constant brightness to the training image\n",
    "\n",
    "# Only run if not using CSV or CSV does not exist\n",
    "if not(use_csv and os.path.exists(training_stem + '_yes_points.csv')):\n",
    "    for i in range(1, imarray_train.shape[0]-1, 1):\n",
    "        for j in range(1, imarray_train.shape[1]-1, 1):\n",
    "            step_x=5 \n",
    "            step_y=5\n",
    "            if mask_train[i,j,0]==1:\n",
    "                if i<step_x:\n",
    "                    step_x=i\n",
    "                elif i>(imarray_train.shape[0]-step_x):\n",
    "                    step_x=imarray_train.shape[0]-i\n",
    "                if j<step_y:\n",
    "                    step_y=j\n",
    "                elif j>(imarray_train.shape[1]-step_y):\n",
    "                    step_y=imarray_train.shape[1]-j\n",
    "    \n",
    "                bright_train_region=np.mean(imarray_train[i-step_x:i+step_x, j-step_y:j+step_y,:])\n",
    "    \n",
    "                for h in [0,1,2]:\n",
    "                    if (ROI_train[i,j,h]-int(bright_train_region-bright_train))<256:\n",
    "                        ROI_train[i,j,h]=ROI_train[i,j,h]-int(bright_train_region-bright_train)\n",
    "                    else:\n",
    "                        ROI_train[i,j,h]=255                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c91f5-61b5-4863-bd58-5860c9e57595",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.application.ipy_interactive = True\n",
    "\n",
    "# Only show Napari if not using CSV\n",
    "if use_csv==False:\n",
    "    viewer_0 = napari.Viewer()\n",
    "    \n",
    "    viewer_0.add_image(imarray_train, name='Original', \n",
    "                    colormap='grey', blending='additive')\n",
    "    viewer_0.add_image(mask_train, name='Violet', \n",
    "                    colormap='green', blending='additive')\n",
    "    viewer_0.add_image(ROI_train, name='ROI', \n",
    "                    colormap='grey', blending='additive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the CSV file with training points exists\n",
    "if use_csv and os.path.exists(training_stem + '_yes_points.csv'):\n",
    "    # Load training points from the CSV file\n",
    "    yes_points = csv_to_matrix(training_stem + '_yes_points.csv')\n",
    "else:\n",
    "    # Initialize an empty list for training points\n",
    "    yes_points = []\n",
    "    \n",
    "    # Create an empty label array for the ROI\n",
    "    ROI_labels = np.zeros(imarray_train.shape)\n",
    "    \n",
    "    # Disable interactive mode for Napari\n",
    "    settings.application.ipy_interactive = False\n",
    "    \n",
    "    # Open the training image in Napari for manual annotation\n",
    "    viewer_c = napari.Viewer()\n",
    "    viewer_c.add_image(imarray_train, name='Original', \n",
    "                       colormap='grey', blending='additive')\n",
    "    \n",
    "    # Callback function to handle shape addition in Napari\n",
    "    def on_shape_added(layer, event):\n",
    "        shapes_layer = event.source\n",
    "\n",
    "    # Add a shapes layer for annotating nuclei\n",
    "    shapes_layer = viewer_c.add_shapes(shape_type=\"circle\", name=\"Nuclei training\")\n",
    "    \n",
    "    # Connect the mouse drag event to the callback function\n",
    "    viewer_c.mouse_drag_callbacks.append(on_shape_added)\n",
    "    \n",
    "    # Run Napari for manual annotation\n",
    "    napari.run()\n",
    "\n",
    "    # Process each annotated shape to extract training points\n",
    "    for s in range(0, np.shape(shapes_layer.data)[0]):\n",
    "        # Get the bounding box of the shape\n",
    "        xmax = np.max(shapes_layer.data[s][:, 0])\n",
    "        ymax = np.max(shapes_layer.data[s][:, 1])\n",
    "        xmin = np.min(shapes_layer.data[s][:, 0])\n",
    "        ymin = np.min(shapes_layer.data[s][:, 1])\n",
    "    \n",
    "        # Calculate ellipse parameters\n",
    "        center = ((xmax + xmin) / 2, (ymax + ymin) / 2)  # Center of the ellipse\n",
    "        semi_axes = (\n",
    "            abs(xmax - xmin) / 2,  # Semi-major axis (height / 2)\n",
    "            abs(ymax - ymin) / 2,  # Semi-minor axis (width / 2)\n",
    "        )\n",
    "        rotation = 0  # Napari's ellipses are axis-aligned by default\n",
    "    \n",
    "        # Get the shape of the training image\n",
    "        image_shape = imarray_train.shape\n",
    "    \n",
    "        # Get the pixel coordinates inside the ellipse\n",
    "        enclosed_pixels = ellipse_pixels(imarray_train, center, semi_axes, rotation, image_shape[:2])\n",
    "\n",
    "        # If the CSV file exists, load the existing training points\n",
    "        if os.path.exists(training_stem + '_yes_points.csv'):\n",
    "            yes_points = csv_to_matrix(training_stem + '_yes_points.csv')\n",
    "    \n",
    "        # Add the RGB values of the enclosed pixels to the training points\n",
    "        for i in range(enclosed_pixels.shape[0]):\n",
    "            x = enclosed_pixels[i][0]\n",
    "            y = enclosed_pixels[i][1]\n",
    "            yes_points.append([ROI_train[x, y, 0], ROI_train[x, y, 1], ROI_train[x, y, 2]])\n",
    "\n",
    "        # Save the updated training points to the CSV file\n",
    "        with open(training_stem + '_yes_points.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(yes_points)\n",
    "\n",
    "# Convert the training points to a NumPy array for further processing\n",
    "yes_points = np.array(yes_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7484dac-80e7-422d-95a2-e41a95cbd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize distribution of training points\n",
    "\n",
    "# Plot RGB scatter plots for training points using plt.figure + add_subplot\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.scatter(yes_points[:,0], yes_points[:,1], c = yes_points/255)\n",
    "ax1.set_xlim((0,256))\n",
    "ax1.set_xlabel(\"RED\")\n",
    "ax1.set_ylim((0,256))\n",
    "ax1.set_ylabel(\"GREEN\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.scatter(yes_points[:,0], yes_points[:,2], c = yes_points/255)\n",
    "ax2.set_xlim((0,256))\n",
    "ax2.set_xlabel(\"RED\")\n",
    "ax2.set_ylim((0,256))\n",
    "ax2.set_ylabel(\"BLUE\")\n",
    "\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.scatter(yes_points[:,1], yes_points[:,2], c = yes_points/255)\n",
    "ax3.set_xlim((0,256))\n",
    "ax3.set_xlabel(\"GREEN\")\n",
    "ax3.set_ylim((0,256))\n",
    "ax3.set_ylabel(\"BLUE\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a45a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polar visualization of Hue vs Saturation/Value\n",
    "yes_points_hsv = rgb_to_hsv(yes_points)\n",
    "theta = np.deg2rad(yes_points_hsv[:, 0])  # Hue in radians\n",
    "r_s = yes_points_hsv[:, 1] / 100.0\n",
    "r_v = yes_points_hsv[:, 2] / 100.0\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1,2,1, projection='polar')\n",
    "sc1 = ax1.scatter(theta, r_s, c=yes_points/255.0, s=18, alpha=0.9)\n",
    "ax1.set_title('Hue vs Saturation (polar)')\n",
    "ax1.set_ylim(0,1)\n",
    "ax1.set_yticks([0.25,0.5,0.75,1.0])\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2, projection='polar')\n",
    "sc2 = ax2.scatter(theta, r_v, c=yes_points/255.0, s=18, alpha=0.9)\n",
    "ax2.set_title('Hue vs Value (polar)')\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_yticks([0,50,100,150,200,255])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9de469-3afa-4dde-a459-45a9fb67a321",
   "metadata": {},
   "source": [
    "## File upload & Calibration\n",
    "1. Set `tiff_file` below to the image you want to analyze.\n",
    "2. Run the cell that loads the image (it prints the image shape).\n",
    "3. The next cell will open Napari for *calibration* â€” draw a line over the image scalebar using the Shapes tool (choose the `Line` shape).\n",
    "   - After drawing the line, make sure the Napari window is focused and press `q` to confirm the measurement.\n",
    "   - If you accidentally close Napari or forget to press `q`, the function will ask whether you want to retry (choose `y` to reopen Napari).\n",
    "4. After confirming the line you'll be prompted to enter the real-world scalebar length in micrometers (Âµm).\n",
    "5. Calibration returns `r_X` and `r_Y` (Âµm/pixel) used for downstream size calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41837bd-55ba-4b37-affd-bc756542813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TIFF file to be processed\n",
    "tiff_file = 'pAC024 D28 uM SafO slide7.jpg'\n",
    "\n",
    "# Extract the file name without the extension\n",
    "tiff_stem = os.path.splitext(os.path.basename(tiff_file))[0]\n",
    "\n",
    "# Open the TIFF file as an image\n",
    "img = Image.open(tiff_file)\n",
    "\n",
    "# Convert the image to a NumPy array for further processing\n",
    "imarray = np.array(img)\n",
    "\n",
    "# Print the shape of the image array (dimensions of the image)\n",
    "print(imarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db40230-4a76-456b-96f1-0dc3ff72d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate image resolution (opens Napari to draw scalebar line)\n",
    "res_px = calibrate_image_resolution(imarray)\n",
    "r_X = res_px\n",
    "r_Y = res_px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a965b-6f9b-40dd-ae6f-4a8bbcedd6a4",
   "metadata": {},
   "source": [
    "## ROI (Region of Interest)\n",
    "This section detects tissue areas by looking for blue-dominant regions (the stained tissue).\n",
    "- Make sure calibration has completed and `r_X`, `r_Y` are set before running this section.\n",
    "- The algorithm scans the image in blocks defined by `step` and flags blocks where the blue channel dominates.\n",
    "- After mask creation the code will remove small holes/objects and apply brightness adjustment based on the training image.\n",
    "- Visual checks: Napari will open and overlay the detected ROI in green; if the ROI looks wrong, tweak `step`, `delta`, `holes_threshold`, or `island_threshold`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835546a-c8d8-478e-ad3e-c4228fa84c9f",
   "metadata": {},
   "source": [
    "Variables: [step] will dictate the size of the square of investigation (smaller values will get a better resolution but slower run), [delta] will dictate the sensitivity of the blue level (smaller values lead to more false positive but higher values lead to more false negative).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68c90f-af3e-4417-acf3-13d1943683ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove alpha channel if present\n",
    "if imarray.shape[2] == 4:\n",
    "    imarray = imarray[:, :, :3]\n",
    "\n",
    "# Create a copy of the original image to define the ROI (Region of Interest)\n",
    "ROI_image = imarray.copy()\n",
    "\n",
    "# Initialize a mask image with the same dimensions as the green channel of the original image\n",
    "mask_image = np.zeros(np.shape(imarray[:, :, 1]))\n",
    "\n",
    "# Iterate through the image in steps to identify blue-dominant regions\n",
    "for i in range(1 + step, imarray.shape[0], step):\n",
    "    for j in range(1 + step, imarray.shape[1], step):\n",
    "        # Check if the blue channel is dominant over both red and green channels\n",
    "        if (\n",
    "            np.mean(imarray[i-step:i+step, j-step:j+step, 0]) >= delta + np.mean(imarray[i-step:i+step, j-step:j+step, 1]) and\n",
    "            np.mean(imarray[i-step:i+step, j-step:j+step, 0]) >= delta + np.mean(imarray[i-step:i+step, j-step:j+step, 2])\n",
    "        ):\n",
    "            # Mark the region as part of the mask\n",
    "            mask_image[i:i+step, j:j+step] = 1\n",
    "\n",
    "# Adjust brightness in ROI based on training image brightness\n",
    "for i in range(1, imarray.shape[0]-1, 1):\n",
    "    for j in range(1, imarray.shape[1]-1, 1):\n",
    "        step_x=5 \n",
    "        step_y=5\n",
    "        if mask_image[i,j]==1:\n",
    "            if i<step_x:\n",
    "                step_x=i\n",
    "            elif i>(imarray.shape[0]-step_x):\n",
    "                step_x=imarray.shape[0]-i\n",
    "            if j<step_y:\n",
    "                step_y=j\n",
    "            elif j>(imarray.shape[1]-step_y):\n",
    "                step_y=imarray.shape[1]-j\n",
    "\n",
    "            bright_image=np.mean(imarray[i-step_x:i+step_x, j-step_y:j+step_y,:])\n",
    "\n",
    "            for h in [0,1,2]:\n",
    "                if (ROI_image[i,j,h]-int(bright_image-bright_train))<256:\n",
    "                    ROI_image[i,j,h]=ROI_image[i,j,h]-int(bright_image-bright_train)\n",
    "                else:\n",
    "                    ROI_image[i,j,h]=255\n",
    "\n",
    "\n",
    "# Fill small holes in the mask to create a more continuous region\n",
    "mask_filled = remove_small_holes(mask_image.astype(int), area_threshold=holes_threshold, connectivity=1)\n",
    "\n",
    "# Remove small isolated objects from the mask\n",
    "mask_filled = remove_small_objects(mask_filled, min_size=island_threshold, connectivity=1)\n",
    "\n",
    "# Apply the mask to the original image to isolate the ROI\n",
    "ROI_image = ROI_image * np.stack([mask_filled] * 3, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ede28d-d731-48ed-9f08-235fc7c48597",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.application.ipy_interactive = True\n",
    "\n",
    "# Visualize original, mask, and ROI in Napari\n",
    "viewer_0 = napari.Viewer()\n",
    "\n",
    "viewer_0.add_image(imarray, name='Original', \n",
    "                colormap='grey', blending='additive')\n",
    "viewer_0.add_image(mask_filled, name='Violet', \n",
    "                colormap='green', blending='additive')\n",
    "viewer_0.add_image(ROI_image, name='ROI', \n",
    "                colormap='grey', blending='additive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edae86-68b0-4f8a-896e-c99018aaad60",
   "metadata": {},
   "source": [
    "After you run the next section, a new Napari window will appear with highlighted in green all the nuclei in the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e118b5-582a-4abc-8ffc-75c9140e1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive mode for Napari\n",
    "settings.application.ipy_interactive = True\n",
    "\n",
    "# Create a copy of the ROI image for processing\n",
    "imarray0 = ROI_image.copy()\n",
    "\n",
    "# Extract the red, green, and blue channels from the ROI image\n",
    "imR = imarray0[:, :, 0]\n",
    "imG = imarray0[:, :, 1]\n",
    "imB = imarray0[:, :, 2]\n",
    "\n",
    "# Convert the training points from RGB to HSV\n",
    "yes_points_hsv = rgb_to_hsv(yes_points)\n",
    "\n",
    "# Scale the HSV training points for better performance in the SVM\n",
    "scaler = StandardScaler()\n",
    "yes_points_scaled = scaler.fit_transform(yes_points_hsv)\n",
    "\n",
    "# Set up the One-Class SVM for anomaly detection\n",
    "sensitivity = 0.8  # Sensitivity level (e.g., 10% of points allowed as outliers)\n",
    "clf = OneClassSVM(kernel='rbf', nu=sensitivity, gamma='scale')\n",
    "clf.fit(yes_points_scaled)\n",
    "\n",
    "# Initialize an empty array for the violet channel\n",
    "imV = np.zeros(np.shape(imR))\n",
    "\n",
    "# Initialize a counter for progress tracking\n",
    "tval = 0\n",
    "\n",
    "# Iterate through the image to classify each pixel\n",
    "for i in range(step, np.shape(imV)[0]):\n",
    "    for j in range(step, np.shape(imV)[1]):\n",
    "        tval += 1\n",
    "        # Process only pixels within the mask\n",
    "        if mask_filled[i, j] > 0:\n",
    "            # Extract RGB values of the current pixel\n",
    "            pR = imR[i, j]\n",
    "            pG = imG[i, j]\n",
    "            pB = imB[i, j]\n",
    "            X = np.array([[pR, pG, pB]])\n",
    "\n",
    "            # Convert the pixel to HSV and scale it\n",
    "            X_hsv = rgb_to_hsv(X)\n",
    "            X_scaled = scaler.transform(X_hsv)\n",
    "\n",
    "            # Classify the pixel using the KDE-based classifier\n",
    "            predictions = classify_points(yes_points, X, bandwidth=20.0)\n",
    "\n",
    "            # Assign the prediction value to the violet channel\n",
    "            if predictions > 0.0:\n",
    "                imV[i, j] = predictions[0]\n",
    "            else:\n",
    "                imV[i, j] = 0\n",
    "        else:\n",
    "            j += step  # Skip to the next step if outside the mask\n",
    "\n",
    "        # Display progress at regular intervals\n",
    "        if (100.0 * tval / (np.shape(imV)[0] * np.shape(imV)[1]) % 1.0 == 0.0):\n",
    "            clear_output(wait=True)\n",
    "            print('PROGRESS ' + str(100.0 * tval / (np.shape(imV)[0] * np.shape(imV)[1])) + ' %')\n",
    "\n",
    "# Visualize the results in Napari\n",
    "viewer_0 = napari.Viewer()\n",
    "\n",
    "# Add the image to the viewer\n",
    "viewer_0.add_image(imarray, name='Original', \n",
    "                colormap='grey', blending='additive')\n",
    "\n",
    "# Add the violet channel (classification results) to the viewer\n",
    "viewer_0.add_image(imV, name='Violet', \n",
    "                   colormap='green', blending='additive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20e504-9bab-44ee-b218-15966098e140",
   "metadata": {},
   "source": [
    "# Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fcd35",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449e2e1-3660-4c10-8fa0-4ce802fdb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer_full = napari.Viewer()\n",
    "viewer_full.add_image(imarray, name='Original', colormap='grey', blending='additive')\n",
    "viewer_full.add_image(imV, name='Violet', colormap='green', blending='additive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ad149",
   "metadata": {},
   "source": [
    "### Step 1: Thresholding \n",
    "Switch from grayscale to binary scale (different brightness to either 0 for background or 1 for nuclei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08217c-5211-4cc2-bcd4-8a72a4c9a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_in=imV\n",
    "\n",
    "# Get all nonzero values in imV (ignoring background)\n",
    "valid_pixels = imV[imV > 0]  \n",
    "\n",
    "# Compute the percentile threshold only for these valid pixels\n",
    "if len(valid_pixels) > 0:  # Ensure there are nonzero pixels\n",
    "    threshold_value = np.percentile(valid_pixels, 95.0)\n",
    "    strong_th_value = np.percentile(valid_pixels, 98.0)\n",
    "    binary_mask = (imV >= threshold_value)  # Keep only the top 5% brightest pixels\n",
    "    strong_b_mask = (imV >= strong_th_value)  # Keep only the top 5% brightest pixels\n",
    "else:\n",
    "    binary_mask = np.zeros_like(imV)  # If no valid pixels, return empty mask\n",
    "    \n",
    "viewer_full.add_image(binary_mask.astype(float), name=\"Thresholded\", colormap=\"blue\", blending=\"additive\")\n",
    "napari.run()\n",
    "im_out=binary_mask.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5c786",
   "metadata": {},
   "source": [
    "### Step 2: Remove Noise & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a404a4-f02e-424c-b099-99d9c167f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_in=im_out\n",
    "binary_mask=im_in\n",
    "\n",
    "# Remove small objects and close gaps\n",
    "binary_mask2 = remove_small_objects(binary_mask, min_size=5)  \n",
    "binary_mask2 = closing(binary_mask2, square(3))  # Fills small holes and connects nearby white regions.\n",
    "\n",
    "strong_b_mask2 = remove_small_objects(strong_b_mask, min_size=5)  \n",
    "strong_b_mask2 = closing(strong_b_mask2, square(3))  # Fills small holes and connects nearby white regions.\n",
    "\n",
    "viewer_full.add_image(binary_mask2.astype(float), name=\"Noise and art. removed\", colormap=\"blue\", blending=\"additive\")\n",
    "napari.run()\n",
    "im_out=binary_mask2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8d517",
   "metadata": {},
   "source": [
    "### Step 3: Remove Artifacts Based on Size and Roundness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d2ca1-f820-4b8a-a067-4fdd6bb84f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_in=im_out\n",
    "binary_mask2=im_in\n",
    "\n",
    "min_roundness = 0.75\n",
    "\n",
    "# Define the minimum and maximum size for nuclei to be considered valid\n",
    "min_nucleus_diam = 3  # Minimum diameter of a nucleus (um)\n",
    "max_nucleus_diam = 20  # Maximum diameter of a nucleus (um)\n",
    "\n",
    "min_nucleus_diam_res = min_nucleus_diam/((r_X+r_Y)/2)\n",
    "max_nucleus_diam_res = max_nucleus_diam/((r_X+r_Y)/2)\n",
    "\n",
    "min_nucleus_size=(min_nucleus_diam/2)*(min_nucleus_diam/2)*np.pi\n",
    "max_nucleus_size=(max_nucleus_diam/2)*(max_nucleus_diam/2)*np.pi\n",
    "\n",
    "# Initialize an empty mask to store the filtered regions\n",
    "filtered_mask = np.zeros_like(binary_mask2)\n",
    "large_nuclei_mask = filtered_mask.copy()\n",
    "\n",
    "# Iterate through each connected region in the labeled binary mask\n",
    "for region in regionprops(label(binary_mask2)):\n",
    "    # Check if the region's area falls within the valid size range\n",
    "    if min_nucleus_size <= region.area <= max_nucleus_size:\n",
    "        if region.perimeter > 0:\n",
    "            # Compute roundness (circularity)\n",
    "            roundness = 4 * np.pi * region.area / (region.perimeter ** 2)\n",
    "\n",
    "            # Keep only regions that are sufficiently round\n",
    "            if roundness >= min_roundness:\n",
    "                # Add the region to the filtered mask\n",
    "                filtered_mask[label(binary_mask2) == region.label] = 1\n",
    "\n",
    "# Iterate through each connected region in the labeled binary mask\n",
    "for region in regionprops(label(strong_b_mask2)):\n",
    "    # Check if the region's area falls within the valid size range\n",
    "    if min_nucleus_size <= region.area <= max_nucleus_size:\n",
    "        if region.perimeter > 0:\n",
    "            # Compute roundness (circularity)\n",
    "            roundness = 4 * np.pi * region.area / (region.perimeter ** 2)\n",
    "\n",
    "            # Keep only regions that are sufficiently round\n",
    "            if roundness >= min_roundness:\n",
    "                # Add the region to the filtered mask\n",
    "                filtered_mask[label(strong_b_mask2) == region.label] = 1\n",
    "\n",
    "viewer_full.add_image(filtered_mask.astype(float), name=\"Size and roundness threshold\", colormap=\"blue\", blending=\"additive\")\n",
    "napari.run()\n",
    "im_out=filtered_mask.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34dc49c-6840-47e7-b872-7a95cfd12ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ðŸ”¹ Step 4: Filter by Size and Roundness\n",
    "# im_in = im_out\n",
    "# segmented_nuclei = im_in\n",
    "\n",
    "# # Define the minimum roundness value (1.0 = perfect circle)\n",
    "# min_roundness = 0.75  # adjust as needed (0.7â€“0.85 works well for nearly round nuclei)\n",
    "\n",
    "# # Initialize an empty mask to store the filtered segments\n",
    "# filtered_segments = np.zeros_like(segmented_nuclei)\n",
    "\n",
    "# # Initialize a counter for assigning new labels to filtered segments\n",
    "# k = 1\n",
    "\n",
    "# # Iterate through each connected region in the segmented nuclei\n",
    "# for region in regionprops(segmented_nuclei):\n",
    "#     if region.perimeter > 0:\n",
    "#         # Compute roundness (circularity)\n",
    "#         roundness = 4 * np.pi * region.area / (region.perimeter ** 2)\n",
    "\n",
    "#         # Keep only regions that are sufficiently round\n",
    "#         if roundness >= min_roundness:\n",
    "#             filtered_segments[segmented_nuclei == region.label] = k\n",
    "#             k += 1\n",
    "\n",
    "# viewer_full.add_labels(filtered_segments, name=\"Filtered by Roundness\")\n",
    "# napari.run()\n",
    "# im_out = filtered_segments.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11392c4",
   "metadata": {},
   "source": [
    "### Step 4: Watershed for Better Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2576442-5e0b-4660-bbaa-f266ccb8e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_in=im_out\n",
    "filtered_mask=im_in\n",
    "\n",
    "# Compute the distance transform of the filtered mask\n",
    "distance = distance_transform_edt(filtered_mask)\n",
    "\n",
    "# Create markers for the watershed algorithm based on the distance transform\n",
    "# Markers are created where the distance is greater than a small fraction of the maximum distance\n",
    "markers = label(distance > 0.0001 * distance.max())\n",
    "\n",
    "# Apply the watershed algorithm to segment nuclei\n",
    "# The negative distance is used to ensure that the watershed grows from the markers\n",
    "segmented_nuclei = watershed(-distance, markers, mask=filtered_mask)\n",
    "\n",
    "viewer_full.add_labels(segmented_nuclei, name=\"Segmented Nuclei\")\n",
    "napari.run()\n",
    "im_out=segmented_nuclei.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583a406-631b-4df5-a014-7654aab4ef09",
   "metadata": {},
   "source": [
    "### Output \n",
    "Creates a .tiff file with multiple pages. P1 is the original image, P2 is the ROI chosen as the tissue, P3 is the detected nuclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed5af4-5640-4a58-94bc-81adc5dbda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_segments=segmented_nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d4253-87d2-4242-b275-de897e0613c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the original image array to an RGB image\n",
    "rgb_im = Image.fromarray(imarray.astype(np.uint8), mode=\"RGB\")\n",
    "\n",
    "# Convert the ROI image array to an RGB image\n",
    "rgb0_im = Image.fromarray(imarray0.astype(np.uint8), mode=\"RGB\")\n",
    "\n",
    "# Initialize an empty RGB array for the filtered segments\n",
    "filtered_segments_rgb = np.zeros((np.shape(filtered_segments)[0], np.shape(filtered_segments)[1], 3))\n",
    "\n",
    "# Generate random colors for each segment label\n",
    "cmd = np.random.rand(np.max(filtered_segments) + 1, 3)\n",
    "cmd[0, :] = [0.0, 0.0, 0.0]  # Ensure the background (label 0) is black\n",
    "\n",
    "# Assign colors to each pixel based on its segment label\n",
    "for i in range(1, np.shape(filtered_segments)[0]):\n",
    "    for j in range(1, np.shape(filtered_segments)[1]):\n",
    "        filtered_segments_rgb[i, j, 0] = int(cmd[filtered_segments[i, j], 0] * 255.0)  # Red channel\n",
    "        filtered_segments_rgb[i, j, 1] = int(cmd[filtered_segments[i, j], 1] * 255.0)  # Green channel\n",
    "        filtered_segments_rgb[i, j, 2] = int(cmd[filtered_segments[i, j], 2] * 255.0)  # Blue channel\n",
    "\n",
    "# Convert the RGB array to uint8 format\n",
    "filtered_segments_rgb = filtered_segments_rgb.astype('uint8')\n",
    "\n",
    "# Convert the filtered segments RGB array to an image\n",
    "b_im = Image.fromarray(filtered_segments_rgb, mode=\"RGB\")\n",
    "\n",
    "# Save the original image, ROI image, and filtered segments image in a single multi-page TIFF file\n",
    "output_path = tiff_stem + \"_output.tiff\"\n",
    "rgb_im.save(output_path, save_all=True, append_images=[rgb0_im, b_im])\n",
    "\n",
    "# Print the output file path\n",
    "print(f\"TIFF file saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791ded5-6ba5-4926-a633-a830f6879410",
   "metadata": {},
   "source": [
    "### QUANTIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c9eeb-cf73-48ff-b210-566762a7dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.ndimage import label, center_of_mass\n",
    "\n",
    "# Get unique labels from the filtered segments (excluding the background label 0)\n",
    "labels = np.unique(filtered_segments)\n",
    "labels = labels[labels != 0]\n",
    "\n",
    "# Print the total number of nuclei detected\n",
    "print('TOTAL NUCLEI ' + str(len(labels+1)))\n",
    "\n",
    "# Compute the centroids (barycenters) of each nucleus\n",
    "barycenters = {l: center_of_mass(filtered_segments == l) for l in labels}\n",
    "\n",
    "# Compute the area of each nucleus in square micrometers\n",
    "areas = {l: np.sum(filtered_segments == l) * r_X * r_Y for l in labels}\n",
    "\n",
    "# Convert barycenters to a dictionary of coordinates in micrometers\n",
    "barycenter_coords = {k: (round(v[0], 2) * r_X, round(v[1], 2) * r_Y) for k, v in barycenters.items()}\n",
    "\n",
    "# Calculate the total area of the image in square micrometers\n",
    "fullA = np.prod(np.shape(mask_image)) * r_X * r_Y\n",
    "\n",
    "# Calculate the total area of the ROI in square micrometers\n",
    "roiA = np.sum(mask_image) * r_X * r_Y\n",
    "\n",
    "# Print the total area of the image and the ROI\n",
    "print(\"TOTAL AREA IMAGE %.2e um2\" % fullA)\n",
    "print(\"TOTAL AREA ROI %.2e um2\" % roiA)\n",
    "\n",
    "# Calculate the concentration of cells in the ROI (cells per square micrometer)\n",
    "roiCON = (len(labels+1)) / roiA\n",
    "roiCONmm=roiCON*1e6\n",
    "\n",
    "# Print the cell concentration in the ROI\n",
    "print(\"CELL CONCENTRATION in ROI %.2e cells/mm2\" % roiCONmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d8187-8d56-4473-b7ef-1c98ca4c715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel workbook\n",
    "workbook = xlsxwriter.Workbook(tiff_stem + '.xlsx')\n",
    "\n",
    "## FORMATS\n",
    "# Define a format for headers (bold text with yellow background)\n",
    "header = workbook.add_format({'bold': True})\n",
    "header.set_bg_color('yellow')\n",
    "\n",
    "# Define a format for floating-point numbers\n",
    "floats = workbook.add_format({'num_format': '0.00'})\n",
    "\n",
    "# Define a format for exponential numbers\n",
    "exp = workbook.add_format()\n",
    "exp.set_num_format(11)\n",
    "\n",
    "## CELLS\n",
    "# Create a worksheet for cell data\n",
    "worksheet_cell = workbook.add_worksheet('Cells')\n",
    "\n",
    "# HEADER\n",
    "# Write the header row for the 'Cells' worksheet\n",
    "worksheet_cell.write_row('A1:E1', ['#ID', 'X [um]', 'Y [um]', 'Area Nuclei [um2]'], header)\n",
    "\n",
    "# CONTENT\n",
    "# Write the data for each nucleus\n",
    "for row, value in enumerate(labels):\n",
    "    worksheet_cell.write(row + 1, 0, value)  # Write nucleus ID\n",
    "    worksheet_cell.write(row + 1, 1, barycenter_coords[value][0], floats)  # Write X coordinate\n",
    "    worksheet_cell.write(row + 1, 2, barycenter_coords[value][1], floats)  # Write Y coordinate\n",
    "    worksheet_cell.write(row + 1, 3, areas[value], floats)  # Write area of the nucleus\n",
    "    clear_output(wait=True)  # Clear the output to show progress\n",
    "    print('NUCLEI ' + str(row + 1) + ' / ' + str(len(labels + 1)))  # Print progress\n",
    "\n",
    "## ROI SHEET\n",
    "# Create a worksheet for ROI data\n",
    "worksheet_ROI = workbook.add_worksheet('ROI')\n",
    "\n",
    "# HEADER\n",
    "# Write the header row for the 'ROI' worksheet\n",
    "worksheet_ROI.write_row('A1:E1', ['# NUCLEI', 'TOT AREA [um2]', 'ROI AREA [um2]', 'CONC NUCLEI in ROI [cells/mm2]'], header)\n",
    "\n",
    "# CONTENT\n",
    "# Write the ROI data\n",
    "worksheet_ROI.write(1, 0, len(labels + 1))  # Write the total number of nuclei\n",
    "worksheet_ROI.write(1, 1, fullA, exp)  # Write the total area of the image\n",
    "worksheet_ROI.write(1, 2, roiA, exp)  # Write the total area of the ROI\n",
    "worksheet_ROI.write(1, 3, roiCONmm, exp)  # Write the cell concentration in the ROI\n",
    "\n",
    "# Close the workbook to save the file\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c18e04-f518-4af6-b31b-9d670de8a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
